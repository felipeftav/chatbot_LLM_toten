import os
import json
import requests
import base64
import io
import traceback
from gtts import gTTS
from flask import Flask, request, jsonify, render_template, make_response, send_from_directory
from flask_cors import CORS
from dotenv import load_dotenv
import google.generativeai as genai


# ============================================================
# üîß CONFIGURA√á√ïES INICIAIS
# ============================================================

# Carrega as vari√°veis do arquivo .env
load_dotenv()

# L√™ todas as chaves de API listadas em GEMINI_API_KEYS (separadas por v√≠rgula)
API_KEYS = [key.strip() for key in os.getenv("GEMINI_API_KEYS", "").split(",") if key.strip()]

if not API_KEYS:
    raise ValueError("A vari√°vel GEMINI_API_KEYS n√£o foi configurada no arquivo .env.")

# Testa todas as chaves e configura a primeira v√°lida
def configure_genai_with_available_key():
    for key in API_KEYS:
        try:
            genai.configure(api_key=key)
            # Teste r√°pido para ver se a chave funciona
            test_model = genai.GenerativeModel("gemini-2.0-flash")
            test_model.generate_content("teste")
            print(f"‚úÖ Chave v√°lida configurada: {key[:8]}...")
            return True
        except Exception as e:
            print(f"‚ùå Chave {key[:8]} inv√°lida ou com limite: {e}")
    raise RuntimeError("üö´ Nenhuma chave Gemini v√°lida dispon√≠vel.")

# Chama a fun√ß√£o antes de criar o modelo LIA
configure_genai_with_available_key()


# ============================================================
# üîó CONEX√ÉO COM O BANCO DE DADOS
# ============================================================

# O Render ir√° popular esta vari√°vel com a URL Interna do seu lia-db
DATABASE_URL = os.getenv("DATABASE_URL") 

if DATABASE_URL:
    try:
        # Tenta conectar ao BD usando a URL
        conn = psycopg2.connect(DATABASE_URL)
        cursor = conn.cursor()
        print("‚úÖ Conex√£o com o banco de dados PostgreSQL estabelecida com sucesso!")
        
        # Exemplo: Executar uma query de teste (opcional)
        # cursor.execute("SELECT version();")
        # db_version = cursor.fetchone()
        # print(f"Vers√£o do PostgreSQL: {db_version[0]}")
        
    except Exception as e:
        print(f"‚ùå Erro ao conectar ao banco de dados: {e}")
        # Uma estrat√©gia comum √© permitir que o app inicie mesmo sem o BD,
        # mas desativar as funcionalidades que dependem dele.
        conn = None 
        cursor = None
else:
    print("‚ö†Ô∏è DATABASE_URL n√£o encontrada. O aplicativo n√£o ter√° acesso ao banco de dados.")
    conn = None 
    cursor = None

def log_message(sender, message_text):
    """Insere uma mensagem no banco de dados."""
    if not conn or not cursor:
        print("‚ö†Ô∏è Banco de dados n√£o dispon√≠vel. Mensagem n√£o foi salva.")
        return
    
    try:
        # Cria a tabela se n√£o existir
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS chat_log (
                id SERIAL PRIMARY KEY,
                sender VARCHAR(10) NOT NULL,
                message TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
        """)
        conn.commit()
        
        # Insere a mensagem
        cursor.execute(
            "INSERT INTO chat_log (sender, message) VALUES (%s, %s);",
            (sender, message_text)
        )
        conn.commit()
        print(f"üíæ Mensagem de {sender} salva no BD!")
    except Exception as e:
        print(f"‚ùå Erro ao salvar mensagem: {e}")
        conn.rollback()



# ============================================================
# ü§ñ CONFIGURA√á√ÉO DO MODELO LIA (Assistente Virtual)
# ============================================================

SYSTEM_INSTRUCTION = """
Voc√™ √© LIA, a assistente virtual oficial do evento Metaday.
Sua miss√£o √© ajudar os participantes com informa√ß√µes sobre o evento de forma amig√°vel, clara e entusiasmada.

--- REGRAS GERAIS ---
- Seja sempre prestativa e positiva.
- Responda de forma concisa e direta.
- Use emojis para deixar a conversa mais leve.
- Fale apenas sobre o Metaday. Se n√£o souber, diga que vai verificar com a organiza√ß√£o.
- N√£o invente informa√ß√µes.

--- INFORMA√á√ïES SOBRE OS PROJETOS (PI) ---

**Gest√£o de Neg√≥cios e Inova√ß√£o (GNI)**
- 1¬∫ Semestre (manh√£ e noite): "N√∫mero Musical" ‚Äì Prof. Clayton Alves Cunha.
- 2¬∫ Semestre (noite): Prof. Clayton Capellari.
- 4¬∫ Semestre (noite): "Pitchs e Impressora 3D" ‚Äì Prof. Sidioney Silveira. Salas 204 e Maker.
- 6¬∫ Semestre (manh√£ e noite): "Consultoria" ‚Äì Prof. F√°tima Leone. Sala multiuso do t√©rreo.

**Marketing (MKT)**
- 1¬∫ Semestre (manh√£): Prof. Ana Lucia. Salas 209, 206 e sala de est√°gio.
- 3¬∫ Semestre (manh√£ e noite): Prof. Ana Lucia. Salas 206 e 207.
- 4¬∫ Semestre (noite): "Podcast" ‚Äì Prof. Isabel. Aqu√°rio, 2¬∫ andar.

**Ci√™ncia de Dados para Neg√≥cios (CDN)**
- 1¬∫ Semestre (tarde): "Dashboard" ‚Äì Prof. Nathane de Castro.
- 2¬∫ Semestre (tarde): "Assistente Virtual LIA" ‚Äì Prof. Carlos Bezerra. (Projeto da pr√≥pria LIA!)

Regras:
- Se o local n√£o for informado, diga que deve confirmar com a organiza√ß√£o.
- Se perguntarem sobre ‚ÄúLIA‚Äù, explique que √© voc√™, criada pelos alunos de Ci√™ncia de Dados. üòÑ
"""

# Cria o modelo Gemini configurado com as instru√ß√µes da LIA
model = genai.GenerativeModel(
    model_name="gemini-2.5-flash",
    system_instruction=SYSTEM_INSTRUCTION,
    generation_config={"temperature": 0.9, "top_p": 1, "top_k": 1, "max_output_tokens": 2048},
    safety_settings=[
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    ],
)

# Inicia o hist√≥rico de conversa
convo = model.start_chat(history=[])

# ============================================================
# üìö RESPOSTAS PR√â-PROGRAMADAS
# ============================================================

EVENT_INFO = {
    "Quais os projetos de GNI?": {
        "text": "O curso de Gest√£o de Neg√≥cios e Inova√ß√£o (GNI) ter√° v√°rias apresenta√ß√µes, como o 'N√∫mero Musical' do 1¬∫ semestre, 'pitchs e demonstra√ß√£o de impressora 3D' do 4¬∫ semestre, e 'atendimento de consultoria' do 6¬∫ semestre. Quer saber o local de algum espec√≠fico?",
        "audio_path": "respostas_pre_gravadas/projetos_gni.mp3"
    },
    "Onde encontro os projetos de Marketing?": {
        "text": "Os projetos de Marketing (MKT) est√£o espalhados pelo evento! Temos apresenta√ß√µes nas salas 209, 206, 207 e um Podcast sendo gravado no Aqu√°rio do 2¬∫ andar. Qual semestre voc√™ procura?",
        "audio_path": "respostas_pre_gravadas/projetos_mkt.mp3"
    },
    "O que √© o projeto da LIA?": {
        "text": "Esse projeto sou eu mesma! Fui desenvolvida pela turma de Ci√™ncia de Dados para Neg√≥cios para ser a assistente virtual oficial do Metaday e ajudar todos voc√™s com informa√ß√µes sobre o evento!",
        "audio_path": "respostas_pre_gravadas/o_que_e_lia.mp3"
    },
    "Onde ser√° a apresenta√ß√£o de Pitch e Impressora 3D?": {
        "text": "A apresenta√ß√£o de pitchs com demonstra√ß√£o de impressora 3D, do 4¬∫ semestre de GNI, acontecer√° na sala 204 e na sala maker. Parece bem interessante!",
        "audio_path": "respostas_pre_gravadas/pitch_impressora.mp3"
    },
    "Tem algum projeto de consultoria?": {
        "text": "Sim! Os alunos do 6¬∫ semestre de GNI, da turma da manh√£, estar√£o oferecendo um atendimento de consultoria na sala multiuso do t√©rreo. √â uma √≥tima oportunidade!",
        "audio_path": "respostas_pre_gravadas/projeto_consultoria.mp3"
    },
    "Onde vai ser o podcast?": {
        "text": "O podcast est√° sendo gravado pelos alunos do 4¬∫ semestre de Marketing no Aqu√°rio do 2¬∫ andar. Vale a pena conferir!",
        "audio_path": "respostas_pre_gravadas/onde_e_podcast.mp3"
    }
}

# ============================================================
# üîä FUN√á√ïES DE CONVERS√ÉO DE TEXTO EM √ÅUDIO (TTS)
# ============================================================

def get_gemini_tts_audio_data(text_to_speak):
    """Gera √°udio com a API de TTS do Gemini usando m√∫ltiplas chaves (fallback autom√°tico)."""
    payload = {
        "contents": [{"parts": [{"text": f"Fale de forma natural e clara, como uma assistente prestativa: {text_to_speak}"}]}],
        "generationConfig": {
            "responseModalities": ["AUDIO"],
            "speechConfig": {"voiceConfig": {"prebuiltVoiceConfig": {"voiceName": "Aoede"}}}
        },
        "model": "gemini-2.5-flash-preview-tts"
    }
    headers = {'Content-Type': 'application/json'}

    for key in API_KEYS:
        try:
            print(f"Tentando gerar √°udio com a chave {key[:8]}...")
            url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent?key={key}"
            response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=30)
            response.raise_for_status()

            result = response.json()
            part = result.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0]
            audio_data = part.get('inlineData', {}).get('data')

            if audio_data:
                print("‚úÖ √Åudio gerado com sucesso via Gemini!")
                return audio_data
        except Exception as e:
            print(f"‚ö†Ô∏è Erro com a chave {key[:8]}: {e}")
    
    raise RuntimeError("üö´ Todas as chaves de API falharam.")

def get_gtts_audio_data(text_to_speak):
    """Fallback local usando gTTS (voz menos natural, mas garantida)."""
    try:
        print("Usando gTTS como alternativa...")
        tts = gTTS(text=f"Fale de forma natural e clara: {text_to_speak}", lang="pt-br")
        buffer = io.BytesIO()
        tts.write_to_fp(buffer)
        return base64.b64encode(buffer.getvalue()).decode("utf-8")
    except Exception as e:
        print(f"ERRO ao gerar TTS com gTTS: {e}")
        return None

def get_tts_audio_data(text_to_speak):
    """Fun√ß√£o principal que tenta o Gemini TTS e usa gTTS se falhar."""
    try:
        return get_gemini_tts_audio_data(text_to_speak)
    except Exception as e:
        print(f"Erro no Gemini TTS: {e}")
        return get_gtts_audio_data(text_to_speak)

# ============================================================
# üåê APLICA√á√ÉO FLASK
# ============================================================

app = Flask(__name__)
CORS(app)

# @app.route('/chat', methods=['POST'])
# def chat():
#     """Rota principal do chatbot LIA."""
#     try:
#         bot_reply_text = ""
#         audio_base64 = None
#         tts_is_enabled = False

#         # Caso o usu√°rio envie √°udio
#         if 'audio_file' in request.files:
#             audio_file = request.files['audio_file']
#             audio_parts = [{"mime_type": audio_file.mimetype, "data": audio_file.read()}]
#             response = convo.send_message(["Responda ao que foi dito neste √°udio.", audio_parts[0]])
#             bot_reply_text = response.text
#             tts_is_enabled = True

#         # Caso o usu√°rio envie JSON
#         elif request.is_json:
#             data = request.json
#             tts_is_enabled = data.get('tts_enabled', False)

#             # Pergunta pr√©-programada
#             if 'preset_question' in data:
#                 question = data['preset_question']
#                 info = EVENT_INFO.get(question)
#                 if info:
#                     bot_reply_text = info["text"]
#                     if tts_is_enabled:
#                         try:
#                             with open(info["audio_path"], "rb") as f:
#                                 audio_base64 = base64.b64encode(f.read()).decode('utf-8')
#                         except FileNotFoundError:
#                             audio_base64 = get_tts_audio_data(bot_reply_text)
#                 else:
#                     bot_reply_text = "Desculpe, n√£o tenho uma resposta para essa pergunta."

#             # Mensagem normal
#             elif 'message' in data:
#                 user_message = data['message']
#                 convo.send_message(user_message)
#                 bot_reply_text = convo.last.text

#         # Gera √°udio se o TTS estiver ativo
#         if audio_base64 is None and tts_is_enabled and bot_reply_text:
#             audio_base64 = get_tts_audio_data(bot_reply_text)

#         return jsonify({
#             "reply": bot_reply_text,
#             "audioData": audio_base64,
#             "presetQuestions": list(EVENT_INFO.keys())
#         })

#     except Exception as e:
#         print(f"Erro no /chat: {e}")
#         traceback.print_exc()
#         return jsonify({"error": "Erro interno no servidor."}), 500

@app.route('/chat', methods=['POST'])
def chat():
    """Rota principal do chatbot LIA, agora com log no BD."""
    bot_reply_text = ""
    audio_base64 = None
    tts_is_enabled = False
    user_message_to_log = None # Vari√°vel para capturar a mensagem do usu√°rio

    try:
        # Caso o usu√°rio envie √°udio
        if 'audio_file' in request.files:
            audio_file = request.files['audio_file']
            audio_parts = [{"mime_type": audio_file.mimetype, "data": audio_file.read()}]
            
            # N√£o logamos o √°udio, mas sim a transcri√ß√£o ou a inten√ß√£o (a resposta do LLM)
            response = convo.send_message(["Responda ao que foi dito neste √°udio.", audio_parts[0]])
            
            # O texto da resposta do bot √© o que ser√° logado
            user_message_to_log = "[√ÅUDIO ENVIADO]" # Marca para o log
            bot_reply_text = response.text
            tts_is_enabled = True # Assume TTS ativado para √°udio

        # Caso o usu√°rio envie JSON (Texto ou Preset)
        elif request.is_json:
            data = request.json
            tts_is_enabled = data.get('tts_enabled', False)

            # Pergunta pr√©-programada
            if 'preset_question' in data:
                question = data['preset_question']
                user_message_to_log = f"[PRESET]: {question}" # Loga como preset
                info = EVENT_INFO.get(question)
                
                if info:
                    bot_reply_text = info["text"]
                    if tts_is_enabled:
                        try:
                            # Tenta ler o √°udio pr√©-gravado
                            with open(info["audio_path"], "rb") as f:
                                audio_base64 = base64.b64encode(f.read()).decode('utf-8')
                        except FileNotFoundError:
                            # Se n√£o encontrar o arquivo, gera o √°udio na hora
                            audio_base64 = get_tts_audio_data(bot_reply_text)
                else:
                    # Se o preset n√£o existir no dict, envia ao LLM como fallback
                    convo.send_message(question)
                    bot_reply_text = convo.last.text

            # Mensagem normal de texto
            elif 'message' in data:
                user_message = data['message']
                user_message_to_log = user_message # Loga a mensagem do usu√°rio
                convo.send_message(user_message)
                bot_reply_text = convo.last.text

        # L√≥gica de Log (Salva a intera√ß√£o ap√≥s a resposta ser gerada)
        if user_message_to_log:
            log_message('user', user_message_to_log)
            log_message('bot', bot_reply_text)

        # Gera √°udio se o TTS estiver ativo e ainda n√£o tiver sido gerado
        if audio_base64 is None and tts_is_enabled and bot_reply_text:
            audio_base64 = get_tts_audio_data(bot_reply_text)

        return jsonify({
            "reply": bot_reply_text,
            "audioData": audio_base64,
            "presetQuestions": list(EVENT_INFO.keys())
        })

    except Exception as e:
        print(f"Erro no /chat: {e}")
        traceback.print_exc()
        return jsonify({"error": "Erro interno no servidor."}), 500


@app.route('/suggest-topic', methods=['GET'])
def suggest_topic():
    """Sugere um t√≥pico curto para iniciar uma conversa."""
    try:
        response = model.generate_content("Sugira um t√≥pico divertido e curto para come√ßar uma conversa.")
        return jsonify({"topic": response.text.strip()})
    except Exception as e:
        return jsonify({"error": f"Erro ao sugerir t√≥pico: {e}"}), 500

@app.route('/summarize', methods=['POST'])
def summarize():
    """Resume o hist√≥rico atual da conversa."""
    try:
        if not convo.history:
            return jsonify({"summary": "Ainda n√£o h√° hist√≥rico de conversa."})
        formatted = "\n".join(
            f"{'Usu√°rio' if m.role == 'user' else 'Assistente'}: {m.parts[0].text}"
            for m in convo.history if m.parts and hasattr(m.parts[0], 'text')
        )
        prompt = f"Resuma a conversa em portugu√™s, de forma breve e objetiva:\n\n{formatted}"
        response = model.generate_content(prompt)
        return jsonify({"summary": response.text})
    except Exception as e:
        return jsonify({"error": f"Erro ao resumir: {e}"}), 500

@app.route('/restart', methods=['POST'])
def restart():
    """Reinicia a conversa (limpa o hist√≥rico)."""
    try:
        convo.history.clear()
        return jsonify({"status": "success", "message": "Conversa reiniciada."})
    except Exception as e:
        return jsonify({"error": f"Erro ao reiniciar: {e}"}), 500

# ============================================================
# üöÄ EXECU√á√ÉO
# ============================================================

# ROTA CORRIGIDA PARA ATIVOS (ASSETS)
# REMOVENDO A MANIPULA√á√ÉO AGRESSIVA DE CACHE HTTP
@app.route("/assets/<path:filename>")
def assets(filename):
    # A remo√ß√£o do cabe√ßalho de Cache-Control agressivo aqui
    # permite que o "cache busting" do JavaScript funcione corretamente.
    return send_from_directory("assets", filename)

# Serve o index.html da raiz
@app.route('/')
def serve_index():
    return send_from_directory('.', 'index.html')

# Serve qualquer outro arquivo est√°tico da raiz (CSS, JS, imagens, etc)
@app.route('/<path:filename>')
def serve_static_files(filename):
    return send_from_directory('.', filename)

if __name__ == '__main__':
    app.run(debug=True, port=5000)